{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This contains the code for getting the BiLSTM embeddings of a sentence or document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "\n",
    "import math\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = ['I like pizza', 'but I hate sushi', 'I am hungry']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Dictionary:\n",
    "    def __init__(self):\n",
    "        self.word2index = {}\n",
    "        self.index2word = {}\n",
    "        \n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2index:\n",
    "            idx = len(self.word2index)\n",
    "            self.word2index[word] = idx\n",
    "            self.index2word[idx] = word\n",
    "        \n",
    "    def add_sentences(self, sentences):\n",
    "        for sent in sentences:\n",
    "            words = word_tokenize(sent)\n",
    "            for word in words:\n",
    "                word = word.lower()\n",
    "                \n",
    "                self.add_word(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = Dictionary()\n",
    "dictionary.add_sentences(sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_indices(sentences, dictionary):\n",
    "    sent_list = []\n",
    "    for sent in sentences:\n",
    "        words = word_tokenize(sent)\n",
    "    \n",
    "        sent_indices = []\n",
    "        for word in words:\n",
    "            word = word.lower()\n",
    "            sent_indices.append(dictionary.word2index[word])\n",
    "        sent_list.append(sent_indices)\n",
    "    return sent_list\n",
    "\n",
    "def batchify(data, batch_size=2, use_cuda=False):\n",
    "    nbatch = math.ceil(len(data)/batch_size)\n",
    "    batches = []\n",
    "    \n",
    "    def list2batch(sent_list):\n",
    "        b_size = len(sent_list)\n",
    "        maxlen = max([len(x) for x in sent_list])\n",
    "        input_tensor = torch.LongTensor(maxlen, b_size).fill_(0)\n",
    "        for idx, s in enumerate(sent_list):\n",
    "            input_tensor[:len(s), idx] = torch.LongTensor(s)\n",
    "        if use_cuda:\n",
    "            input_tensor = input_tensor.cuda()\n",
    "        return input_tensor\n",
    "    \n",
    "    for b_id in range(nbatch):\n",
    "        b_data = data[(b_id * batch_size) : (b_id+1) * batch_size ]\n",
    "        input_tensor = list2batch(b_data)\n",
    "        batches.append(input_tensor)\n",
    "    return batches\n",
    "\n",
    "sent_idx_data = get_indices(sentences, dictionary)\n",
    "print(sent_idx_data)\n",
    "batches = batchify(sent_idx_data)\n",
    "print(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BLSTMEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, batch_size, word_emb_dim, encoder_dim, vocab_size, num_judges, num_layers=1, dropout=0.3):\n",
    "        super(BLSTMEncoder, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.word_emb_dim = word_emb_dim\n",
    "        self.enc_lstm_dim = encoder_dim\n",
    "        self.pool_type = 'max'\n",
    "        self.dpout_model = dropout\n",
    "        self.num_layers = num_layers\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "        self.embed = nn.Embedding(vocab_size, word_emb_dim)\n",
    "        self.enc_lstm = nn.LSTM(self.word_emb_dim, self.enc_lstm_dim, 1,\n",
    "                                bidirectional=True, batch_first = True,dropout=self.dpout_model)\n",
    "        self.fc = nn.Linear(self.enc_lstm_dim*2, num_judges)  # 2 for bidirection\n",
    "    \n",
    "        #self.init_embedding()\n",
    "\n",
    "\n",
    "    def forward(self, x, evaluation_mode = False):\n",
    "        # Set initial states\n",
    "        memory_states = (Variable(torch.zeros(self.num_layers*2, len(x), self.enc_lstm_dim), requires_grad=evaluation_mode),\n",
    "              Variable(torch.zeros(self.num_layers*2, len(x), self.enc_lstm_dim), requires_grad=evaluation_mode))\n",
    "        \n",
    "        emb = self.embed(Variable(x, requires_grad=evaluation_mode)) #get word embedding\n",
    "        emb = self.drop(emb)\n",
    "        # Forward propagate LSTM\n",
    "        out, hidden = self.enc_lstm(emb, memory_states)\n",
    "        \n",
    "        \n",
    "        # max pooling\n",
    "        out = torch.max(out, 0)[0]\n",
    "        \n",
    "        # classification of judge\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def init_embedding(self):\n",
    "        initrange = 0.1\n",
    "        self.enc_lstm.weight.data.uniform_(-initrange, initrange)\n",
    "        self.enc_lstm.bias.data.fill_(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the BiLSTM model\n",
    "BATCH_SIZE = len(batches[-1])\n",
    "WORD_EMB_DIM = 5\n",
    "ENCODER_DIM = 10\n",
    "VOCAB_SIZE = len(dictionary.word2index)\n",
    "JUDGE_NUM = 100\n",
    "model = BLSTMEncoder(BATCH_SIZE, WORD_EMB_DIM, ENCODER_DIM, VOCAB_SIZE, JUDGE_NUM)\n",
    "\n",
    "## checking if the model is producing the embedding\n",
    "# sent_output = model(batches[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "\n",
       "Columns 0 to 9 \n",
       "-0.0075  0.1641  0.0566  0.2765  0.1459 -0.0716  0.1065  0.1346  0.0459 -0.0001\n",
       "\n",
       "Columns 10 to 19 \n",
       " 0.0680  0.0038  0.3045  0.1725  0.0591  0.1864  0.1845  0.0679  0.2022  0.1409\n",
       "[torch.FloatTensor of size 1x20]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)\n",
    "    \n",
    "# Train the Model \n",
    "for epoch in range(num_epochs):\n",
    "    for i, batch in enumerate(batches):\n",
    "        # Forward + Backward + Optimize\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [%d/%d], Step [%d/%d], Loss: %.4f' \n",
    "                   %(epoch+1, num_epochs, i+1, len(train_dataset)//batch_size, loss.data[0]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
